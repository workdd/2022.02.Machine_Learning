{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install spacy\n",
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow_hub\n",
        "!pip install scipy\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r4iviSToxZDi"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 import\n",
        "import json\n",
        "import os\n",
        "import openai\n",
        "import random\n",
        "import spacy\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "from scipy import spatial\n",
        "from scipy.spatial import distance\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# OpenAI 주소 : https://openai.com/api/\n",
        "# API_KEY의 경우 OpenAI 내 개인 계정에서 생성하여 사용할 수 있다.\n",
        "# 위 주소에서 View API keys를 이용하여 아래를 채워 사용해야 한다.\n",
        "# 환경변수 설정 필요\n",
        "# export OPENAI_API_KEY = \" 자신의 api-key \"\n",
        "# 아래 빈 부분에 자신의 api-key를 넣어야 사용할 수 있다.\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " The Paraphraser and the Translator: Two Convolutional Modules for Knowledge Transfer \n"
          ]
        }
      ],
      "source": [
        "# 실제로 활용해보기\n",
        "# abstract 를 원하는 논문의 abstract로 채우고 실행 시 활용 가능\n",
        "\n",
        "abstract = \"Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher’s knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "        model=\"davinci:ft-personal-2022-12-11-02-49-21\", \n",
        "        prompt=f\"Generate title with this : {abstract}\",\n",
        "        temperature=0.1, \n",
        "        max_tokens=1024)\n",
        "    \n",
        "res = response[\"choices\"][0][\"text\"].split(\"END\")[0]\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2174796it [00:52, 41697.45it/s]\n"
          ]
        }
      ],
      "source": [
        "# 실제로 활용해보기\n",
        "# abstract를 Dataset의 abstract를 이용해서 실행 시 활용 가능\n",
        "\n",
        "t = []\n",
        "a = []\n",
        "\n",
        "# 테스트를 위한 Dataset 생성\n",
        "with open(\"arxiv_data/arxiv-metadata-oai-snapshot.json\", 'r') as f:  # arxiv dataset 읽어오기\n",
        "    for i, entry in enumerate(tqdm(f)):\n",
        "        data = dict(json.loads(entry))\n",
        "        if \"cs.AI\" in data[\"categories\"]:\n",
        "            t.append(data[\"title\"])\n",
        "            a.append(data[\"abstract\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: Large-Scale Automatic Labeling of Video Events with Verbs Based on\n",
            "  Event-Participant Interaction, \n",
            "Generated:  Event Recognition with Spatiotemporal Verbs \n"
          ]
        }
      ],
      "source": [
        "# 원하는 Index 선택\n",
        "idx = 2345\n",
        "\n",
        "response = openai.Completion.create(\n",
        "        model=\"davinci:ft-personal-2022-12-11-02-49-21\", \n",
        "        prompt=f\"Generate title with this : {a[idx]}\",\n",
        "        temperature=0, \n",
        "        max_tokens=1024)\n",
        "    \n",
        "res = response[\"choices\"][0][\"text\"].split(\"END\")[0]\n",
        "\n",
        "print(f\"Original: {t[idx]}, \\nGenerated:{res}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLvXWw41x5p6"
      },
      "outputs": [],
      "source": [
        "fine_tune = []\n",
        "\n",
        "# Fine tuning을 위한 Dataset 생성\n",
        "with open(\"arxiv_data/arxiv-metadata-oai-snapshot.json\", 'r') as f:  # arxiv dataset 읽어오기\n",
        "    for i, entry in enumerate(f):\n",
        "        data = dict(json.loads(entry))\n",
        "        if \"cs.AI\" in data[\"categories\"]:       # data의 category 중 cs.AI만 뽑기\n",
        "            tmp = {}\n",
        "            abs = data[\"abstract\"]\n",
        "            \n",
        "            # Prepare Data 내 prompt라는 key에 대한 value로 abstract를 설정\n",
        "            tmp[\"prompt\"] = f\"Generate title with this : {abs}\"\n",
        "            \n",
        "            # Prepare Data 내 completion이라는 key에 대한 value로 title을 설정\n",
        "            tmp[\"completion\"] = data[\"title\"]\n",
        "            \n",
        "            fine_tune.append(tmp)\n",
        "\n",
        "# arxiv data 전체를 사용 시 fine tuning 시간이 오래걸리므로 데이터를 500개로 한정            \n",
        "fine_tune_train = fine_tune[:500]\n",
        "\n",
        "# Fune tuning을 위한 Json Lines Dataset 생성 (Fune tuning을 하기 위해선 json이 아니라 jsonl 확장자가 필요)\n",
        "with open(\"arxiv_data/finetune_traindata.jsonl\", 'w') as f:\n",
        "    for i in fine_tune_train: f.write(json.dumps(i) + \"\\n\")\n",
        "    \n",
        "test = []\n",
        "gt = []\n",
        "\n",
        "# Test를 위한 Data 생성 (Testset 생성)\n",
        "with open(\"arxiv_data/arxiv-metadata-oai-snapshot.json\", 'r') as f:\n",
        "    for i, entry in enumerate(f):\n",
        "        data = dict(json.loads(entry))\n",
        "        if \"cs.AI\" in data[\"categories\"]:\n",
        "            test.append(data[\"abstract\"])\n",
        "            gt.append(data[\"title\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 샘플 생성            \n",
        "idx = []\n",
        "\n",
        "for i in range(0, 55185):\n",
        "    idx.append(i)\n",
        "\n",
        "sample = random.sample(idx, 100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 기존 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 평균 성능 측정 (Finetuning 시 사용하지 않은 데이터 이용)\n",
        "test_set = test[500:]\n",
        "gt_set = gt[500:]\n",
        "default_distance = 0\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "for i, idx in enumerate(tqdm(sample)):\n",
        "    t = test_set[idx]\n",
        "    titles = []\n",
        "    \n",
        "    # 기존 모델에서 생성한 제목 \n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\", \n",
        "        prompt=f\"Generate title with this : \\n\\n{t}\",\n",
        "        temperature=0, \n",
        "        max_tokens=1024)\n",
        "    \n",
        "    res = response[\"choices\"][0][\"text\"]\n",
        "    \n",
        "    titles.append(gt_set[idx])\n",
        "    titles.append(res)\n",
        "    \n",
        "    embeddings = embed(titles)\n",
        "    \n",
        "    # Universal Sentence Encoding을 이용하여 유사도 측정\n",
        "    default_distance += (1 - distance.cosine(embeddings[0], embeddings[1]))\n",
        "    print(f\"Current Distance = {default_distance / (i+1)}\")\n",
        "\n",
        "# 누적된 유사도를 sample 개수로 나누어 평균적인 유사도를 측정\n",
        "default_distance /= len(sample)\n",
        "\n",
        "print(default_distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default Model Similarity using Universal Sentene Encoding: 0.5018037787824869\n"
          ]
        }
      ],
      "source": [
        "print(f\"Default Model Similarity using Universal Sentence Encoding: {default_distance}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine Tuning 한 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 평균 성능 측정 (Finetuning 시 사용하지 않은 데이터 이용)\n",
        "test_set = test[500:]\n",
        "gt_set = gt[500:]\n",
        "default_distance = 0\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "for i, idx in enumerate(tqdm(sample)):\n",
        "    t = test_set[idx]\n",
        "    titles = []\n",
        "    \n",
        "    # Fine Tuning 한 모델을 이용하여 제목 생성\n",
        "    response = openai.Completion.create(\n",
        "        model=\"davinci:ft-personal-2022-12-11-02-49-21\", \n",
        "        prompt=f\"Generate title with this : {t}\",\n",
        "        temperature=0, \n",
        "        max_tokens=1024)\n",
        "    \n",
        "    res = response[\"choices\"][0][\"text\"].split(\"END\")[0]\n",
        "    \n",
        "    titles.append(gt_set[idx])\n",
        "    titles.append(res)\n",
        "    \n",
        "    embeddings = embed(titles)\n",
        "    \n",
        "    # Universal Sentence Encoding을 이용하여 유사도 측정\n",
        "    default_distance += (1 - distance.cosine(embeddings[0], embeddings[1]))\n",
        "    print(f\"Current Distance = {default_distance / (i+1)}\")\n",
        "\n",
        "# 누적된 유사도를 sample 개수로 나누어 평균적인 유사도를 측정\n",
        "default_distance /= len(sample)\n",
        "\n",
        "print(default_distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine Tuned Model Similarity using Universal Sentene Encoding: 0.527911923378706\n"
          ]
        }
      ],
      "source": [
        "print(f\"Fine Tuned Model Similarity using Universal Sentence Encoding: {default_distance}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "3bae2c9f0b5d0824fa457013c81d2f1ed9204a8b043f5ee9b3b77804dba2bc1f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
